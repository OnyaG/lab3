# Лабораторная работа №3

## Исполнители

Обучающиеся группы 449м: Евсеенкова Кристина Денисовна, Старков Силантий Денисович, Ташкова Анна Юрьевна

## Цель работы

Познакомиться с задачей генерации текста.

## Оглавление

*   [1. Теоретическая база](#1-теоретическая-база)
    *   [1.1 Архитектура трансформеров](#11-архитектура-трансформеров)
    *   [1.2 Эмбеддинги](#12-эмбеддинги)
*   [2. Эмбеддинг ALBERT](#2-эмбеддинг-albert)
    *   [2.1 Что такое ALBERT?](#21-что-такое-albert)
    *   [2.2 Основные особенности ALBERT](#22-основные-особенности-albert)
    *   [2.3 Применение ALBERT](#23-применение-albert)
    *   [2.4 Преимущества и недостатки](#24-преимущества-и-недостатки)
*   [3. Результаты работы и тестирования системы](#3-результаты-работы-и-тестирования-системы)
*   [4. Выводы по работе](#4-выводы-по-работе)
*   [Список использованных источников](#список-использованных-источников)

# ALBERT (A Lite BERT)

ALBERT — это улучшенная версия модели BERT (Bidirectional Encoder Representations from Transformers), разработанная для повышения эффективности и снижения вычислительных требований, сохраняя при этом высокую производительность на задачах обработки естественного языка. Эта модель была представлена в 2019 году и быстро завоевала популярность благодаря своим инновационным подходам к архитектуре и обучению.

## 1. Теоретическая база

### 1.1 Архитектура трансформеров

Трансформеры — это архитектура нейронных сетей, предложенная в 2017 году в статье *"Attention is All You Need"*. Они предназначены для обработки последовательностей данных, таких как текст, и основываются на механизме внимания, который позволяет моделям учитывать контекст при обработке входных данных. Основные компоненты трансформеров:

- **Механизм внимания**: Позволяет модели фокусироваться на различных частях входной последовательности, взвешивая важность каждого слова в контексте других.
  
- **Многоуровневая структура**: Состоит из нескольких слоев, каждый из которых включает механизмы внимания и полносвязные слои.

- **Позиционные эмбеддинги**: Добавляют информацию о позиции каждого слова в последовательности.

- **Нормализация и остаточные соединения**: Используются для улучшения сходимости и предотвращения исчезновения градиента.
![-y7ztlhyq5_817b89qhjjikwopk](https://github.com/user-attachments/assets/10520e09-880c-451e-93f7-aa6dc5b59e37)

<p align="center">  
Рисунок 1 - Архитектура трансформеров
</p>

### 1.2 Эмбеддинги

Эмбеддинги — это представления слов или токенов в виде векторов фиксированной размерности. Они позволяют моделям захватывать семантические и синтаксические отношения между словами. Основные аспекты теории эмбеддингов:

- **Обучение эмбеддингов**: Эмбеддинги могут быть обучены с нуля или предобучены на больших объемах текстовых данных.

- **Контекстуальные эмбеддинги**: Создают разные представления для одного и того же слова в зависимости от его контекста.

- **Применение эмбеддингов**: Используются в задачах классификации текста, анализа настроений, ответа на вопросы и генерации текста.

![image](https://github.com/user-attachments/assets/ee61de78-4a0c-4448-9e9d-d2841e8f9fc0)

<p align="center">  
Рисунок 2 - Эмбеддинги. Предложение, конвертированное в последовательность токенов
</p>

## 2. Эмбеддинг ALBERT

### 2.1 Что такое ALBERT?

ALBERT (A Lite BERT) — это улучшенная версия модели BERT, разработанная для повышения эффективности и уменьшения требований к вычислительным ресурсам. Основная цель ALBERT заключается в том, чтобы сохранить высокую производительность на задачах обработки естественного языка при значительно меньших вычислительных затратах.

### 2.2 Основные особенности ALBERT

1. **Разделение параметров**: Позволяет значительно сократить количество параметров в модели.

2. **Многоуровневая архитектура**: Сохраняет способность к пониманию сложных языковых структур.

3. **Обучение на больших объемах данных**: ALBERT использует предобучение на больших корпусах текстов.

4. **Контекстуальные эмбеддинги**: Создает разные представления для одного и того же слова в зависимости от контекста.
   
![The-architecture-of-the-ALBERT-model-in-our-task](https://github.com/user-attachments/assets/f48019f5-bd74-416c-87ee-9c1ede627f2a)

<p align="center">  
Рисунок 3 - ALBERT. Иллюстрация, демонстрирующая архитектуру ALBERT и его ключевые особенности
</p>

### 2.3 Применение ALBERT

ALBERT можно использовать для множества задач в области обработки естественного языка:

- **Классификация текста**: Эффективно классифицирует текстовые документы по темам и жанрам.

- **Ответ на вопросы**: Обрабатывает запросы и извлекает информацию из текстов.

- **Анализ настроений**: Определяет эмоциональную окраску текста.

- **Генерация текста**: Создает связный и осмысленный текст.

### 2.4 Преимущества и недостатки

**Преимущества**:
- **Более легковесная модель**: ALBERT значительно легче, чем BERT.
- **Высокая производительность**: Конкурентоспособные результаты на различных задачах NLP.
- **Снижение времени обучения**: Требует меньше времени для обучения.

**Недостатки**:
- **Время обучения**: Может потребовать больше времени на обучение по сравнению с другими моделями.
- **Точность**: Может не достигать такой же точности, как более крупные модели на сложных задачах.

## 3. Результаты работы и тестирования системы 

В результате выполнения лабораторной работы были получены эмбеддинги Albert и построена нейронная сеть на их основе. Оценка качества модели на тесте (Precision/Recall/F1Score) приведена на рисунке 4.

<p align="center">  
Рисунок 4 - Оценка качества модели
</p>

Результаты обучения модели представлены на рисунке 5.

<p align="center">  
Рисунок 5 - Результаты
</p>

## Выводы по работе

В результате выполнения лабораторной работы ...

## Список использованных источников

1. Хайкин, С. Нейронные сети: полный курс / С. Хайкин. – 2-е изд., испр. – Москва: Вильямс, 2006. – 1104 с.
Гудфеллоу, А., Бенджио, И., Курвилль, А. Глубокое обучение / А. Гудфеллоу, И. Бенджио, А. Курвилль; пер. с англ. – Москва: ДМК Пресс, 2018. – 652 с.
2. Рассел, С., Норвиг, П. Искусственный интеллект: современный подход / С. Рассел, П. Норвиг; пер. с англ. – 2-е изд. – Москва: Вильямс, 2006. – 1408 с.
3. Ivanov, D.A. Analysis and forecasting of time series using neural networks / D.A. Ivanov // Information Technologies. – 2018. – No 4. – Pp. 25-29.
4. Petrov, V.V., Sidorov, S.I. Development of an expert system for disease diagnosis / V.V. Petrov, S.I. Sidorov // Artificial Intelligence and Decision Making. – 2020. – No 2. – Pp. 15-22.
Sergeev, A.S. Application of genetic algorithms for optimizing neural network parameters / A.S. Sergeev // Control Problems. – 2019. – No 5. – Pp. 30-35.
5. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K., BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding // arXiv preprint arXiv:1810.04805 (2018).
6. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, K., & Soricut, R., ALBERT: A Lite BERT for Self-supervised Learning of Language Representations // arXiv preprint arXiv:1909.11942 (2019).
