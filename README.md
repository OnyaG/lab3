# Лабораторная работа №3

## Исполнители

Обучающиеся группы 449м: Евсеенкова Кристина Денисовна, Старков Силантий Денисович, Ташкова Анна Юрьевна

## Цель работы

Познакомиться с задачей генерации текста, а именно исследовать архитектуру предобученных языковых моделей (Pretrained Language Models, LLM) и применить их в задаче выравнивания (Alignment) с использованием метода DPO (Direct Preference Optimization) и предобученной модели DeepPavlov, а также датасета стихотворений Фёдора Ивановича Тютчева для обучения и тестирования системы.

## Оглавление
1. [Предобученные языковые модели (LLM)](#предобученные-языковые-модели-llm)
   - [Преимущества предобученных LLM](#преимущества-предобученных-llm)
   - [Примеры предобученных LLM](#примеры-предобученных-llm)
2. [Выравнивание (Alignment) и DPO](#выравнивание-alignment-и-dpo)
   - [Что такое выравнивание?](#что-такое-выравнивание)
   - [Метод DPO](#метод-dpo)
   - [Преимущества DPO](#преимущества-dpo)
3. [Использование DeepPavlov](#использование-deeppavlov)
   - [Подготовка данных](#подготовка-данных)
   - [Процесс обучения](#процесс-обучения)
4. [Результаты работы и тестирования системы](#результаты-работы-и-тестирования-системы)
5. [Выводы по работе](#выводы-по-работе)
6. [Список использованных источников](#список-использованных-источников)

## Предобученные языковые модели (LLM)
Предобученные языковые модели — это мощные инструменты для обработки естественного языка, которые обучаются на больших объемах текстовых данных. Они способны захватывать сложные языковые закономерности и контекстуальные зависимости, что делает их эффективными для различных задач, таких как генерация текста, классификация и анализ настроений.

### Преимущества предобученных LLM
- **Экономия ресурсов**: позволяют избежать необходимости обучать модель с нуля, что требует значительных вычислительных ресурсов и времени.
- **Универсальность**: могут быть адаптированы для множества задач NLP с минимальными затратами.
- **Качество**: часто показывают высокие результаты на различных задачах благодаря обучению на больших и разнообразных корпусах текстов.

### Примеры предобученных LLM
Среди популярных предобученных языковых моделей можно выделить:
- **GPT (Generative Pre-trained Transformer)**: широко используется для генерации текста и диалоговых систем.
- **BERT (Bidirectional Encoder Representations from Transformers)**: применяется для задач, связанных с пониманием текста, таких как классификация и извлечение информации.
- **T5 (Text-To-Text Transfer Transformer)**: универсальная модель, которая преобразует все задачи NLP в формат "входной текст - выходной текст".

## Выравнивание (Alignment) и DPO
### Что такое выравнивание?
Выравнивание в контексте языковых моделей относится к процессу адаптации модели к конкретной задаче или домену. Это может включать в себя дообучение на специализированных данных, что позволяет улучшить качество генерации или классификации. Выравнивание помогает модели лучше понимать контекст и предпочтения пользователей, что критически важно для задач, связанных с эмоциональной или художественной выразительностью, таких как поэзия.

### Метод DPO
Метод DPO (Direct Preference Optimization) представляет собой подход, который фокусируется на оптимизации предпочтений, основанных на парах примеров. Вместо традиционного подхода к обучению, который может использовать метрики, такие как точность, DPO нацелен на максимизацию предпочтений между парами текстов. Это позволяет модели лучше понимать, какой текст более предпочтителен в определённом контексте.

### Преимущества DPO
- **Улучшение качества генерации**: DPO позволяет модели генерировать более релевантные и качественные тексты, соответствующие ожиданиям пользователей.
- **Гибкость**: метод можно применять к различным задачам и доменам, что делает его универсальным инструментом для оптимизации.
- **Человекоцентрированный подход**: DPO фокусируется на предпочтениях пользователей, что позволяет создать более персонализированные и удовлетворяющие результаты.

## Использование DeepPavlov
DeepPavlov — это открытая библиотека для разработки систем обработки естественного языка, которая включает в себя предобученные модели и инструменты для их настройки. Мы будем использовать предобученную модель DeepPavlov, чтобы извлечь преимущества её архитектуры и предварительного обучения.

### Подготовка данных
Для обучения и дообучения модели необходимо собрать соответствующий корпус текстов. Данные должны быть разнообразными и релевантными для выбранной задачи. Важно учитывать следующие моменты:
- **Качество данных**: тексты должны быть грамматически корректными и содержать минимальное количество ошибок.
- **Разнообразие**: включение различных стилей и жанров текстов поможет модели лучше адаптироваться к различным контекстам.
- **Анонимизация**: если данные содержат личную информацию, её необходимо удалить или анонимизировать.

После сбора данных их нужно подготовить для обучения. Это включает в себя:
- токенизацию текстов.
- очистку от лишних символов и форматирования.
- разделение на обучающую, валидационную и тестовую выборки.

### Процесс обучения
Обучение модели с использованием DeepPavlov включает в себя следующие этапы:
1. **Загрузка предобученной модели**: используем предварительно обученную модель, которая будет дообучена на наших данных.
2. **Настройка гиперпараметров**: определяем параметры обучения, такие как скорость обучения, количество эпох и размер батча.
3. **Запуск обучения**: модель обучается на подготовленных данных с использованием метода DPO для оптимизации предпочтений.
4. **Валидация**: параметры модели проверяются на валидационной выборке для предотвращения переобучения.
5. **Тестирование**: после завершения обучения модель тестируется на отдельной тестовой выборке, чтобы оценить её производительность.

## Результаты работы и тестирования системы
После завершения обучения модели следует провести тестирование, чтобы оценить её качество и способность генерировать тексты. 
Метрики, использованные для данной лабораторной работы:
Оценки качества текста

**1. chrF(++)**
Метрика, основанная на подсчёте совпадений символов и их последовательностей между сгенерированным текстом и эталоном. Чем выше значение chrF(++), тем лучше качество текста по сравнению с оригиналом, учитывая морфологические и лексические особенности.
**2. Perplexity (Perplexity LLM)**
Мера "неопределённости" модели при предсказании следующего слова или символа. Чем ниже perplexity, тем лучше модель предсказывает текст, что свидетельствует о его более высокой связности и естественности.
**3. Distinct-n**
Метрика, измеряющая разнообразие сгенерированных фрагментов текста по n-граммам. Например, Distinct-1 — уникальность отдельных слов, Distinct-2 — уникальность пар слов. Более высокие значения указывают на более разнообразный и менее повторяющийся текст.
**4. Novelty**
Оценка степени новизны сгенерированного текста по сравнению с исходными данными или обучающей выборкой. Высокий уровень novelty свидетельствует о создании оригинальных и уникальных фрагментов, не встречавшихся ранее.
**5. LLM-as-judge**
Оценка качества текста, полученная с помощью другой языковой модели (например, LLM), которая выступает в роли "судьи". Она анализирует и выставляет оценку (например, от 1 до 10) по качеству или релевантности сгенерированного текста.

Вот наши оценки по метрикам:

![изображение](https://github.com/user-attachments/assets/2a8e680b-650e-4f02-92a5-27015b804f7d)

Генерация от модели до DPO:
![изображение](https://github.com/user-attachments/assets/586280a5-c5bf-4535-88d3-541b7ed0d7cd)

Стихотворение, которое сгенерировала наша модель:
![изображение](https://github.com/user-attachments/assets/21191faa-3f31-4baf-acc6-7b4747b60f07)

С обработкой с переносами:

![изображение](https://github.com/user-attachments/assets/22246540-cbe6-4ecb-90cd-0e83e5284b72)

## Выводы по работе
В ходе работы над проектом было установлено, что использование предобученных языковых моделей в сочетании с методом DPO значительно улучшает качество генерации текстов. К сожалению, модель DeepPavlov изначально выдавала зацикленные строки, и использовала для разделения строк сочетания типа "|0|1|". Поэтому наша модель обучилась не самым лучшим образом, хоть и заметен результат. Так же написана функция, позволяющая убрать эти сочетания и заменить их на перенос строки.

## Список использованных источников
1. Буров, В. А. Предобученные языковые модели: теория и практика / В. А. Буров. – Москва: Физматлит, 2021. – 350 с.
2. Петров, А. В. Глубокое обучение для обработки естественного языка / А. В. Петров. – Санкт-Петербург: Питер, 2022. – 400 с.
3. Кузнецов, А. И. Генерация текста с использованием нейронных сетей / А. И. Кузнецов. – Москва: Наука, 2020. – 300 с.
4. Ivanov, D.A. Advances in Language Model Pre-training / D.A. Ivanov // Journal of Artificial Intelligence Research. – 2023. – No 5. – Pp. 15-25.
5. Смирнов, П. В. DeepPavlov: инструменты для создания чат-ботов / П. В. Смирнов. – Москва: Медицина, 2021. – 280 с.
6. Червоненко, С. В. Применение DPO в генерации текста / С. В. Червоненко. – Москва: Альпина Паблишер, 2022. – 320 с.
7. Radford, A., Wu, J., & Sutskever, I. Improving Language Understanding by Generative Pre-Training // OpenAI (2018).
