{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2157fae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5d6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных из JSON файла\n",
    "def load_poems(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236d5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для обучения\n",
    "def prepare_dataset(poems):\n",
    "    texts = []\n",
    "    for poem in poems.values():\n",
    "        texts.append(\"\\n\".join(poem))  # Объединяем строки стихотворения в один текст\n",
    "    return Dataset.from_dict({\"text\": texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5f8490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics):\n",
    "    plt.plot(metrics['train_loss'], label='Train Loss')\n",
    "    plt.plot(metrics['eval_loss'], label='Eval Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    plt.title('Training and Evaluation Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00707f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Загрузка предобученной модели DeepPavlov GPT-2\n",
    "model_name = \"DeepPavlov/rudialogpt3_medium_based_on_gpt2_v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c1b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 368/368 [00:00<00:00, 3067.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Загрузка данных\n",
    "poems = load_poems('data/poems.json')\n",
    "\n",
    "\n",
    "# Токенизация\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "    \n",
    "dataset = prepare_dataset(poems)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d5ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "delimiter_array = [\n",
    "    \"|0|\", \"|1|\", \"|2|\", \"|3|\",\n",
    "    \"|0|0|\", \"|0|1|\", \"|0|2|\", \"|0|3|\",\n",
    "    \"|1|0|\", \"|1|1|\", \"|1|2|\", \"|1|3|\",\n",
    "    \"|2|0|\", \"|2|1|\", \"|2|2|\", \"|2|3|\",\n",
    "    \"|3|0|\", \"|3|1|\", \"|3|2|\", \"|3|3|\"\n",
    "]\n",
    "\n",
    "def generate_dispreferred_poem(model, input_ids, tokenizer):\n",
    "    preferred_poem = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    lines = preferred_poem.split('/n')\n",
    "    lines = [line for line in lines if random.random() > 0.1]\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        words = [word for word in words if random.random() > 0.05]\n",
    "        line = ' '.join(words)\n",
    "        \n",
    "    result_lines = []\n",
    "\n",
    "    lines.append(\"\")\n",
    "    skip = 1\n",
    "    last_string = \"\"\n",
    "    for line in lines:\n",
    "\n",
    "        if skip==0 and random.random() < 0.4:\n",
    "            combined_line = last_string.replace('\\n', random.choice(delimiter_array)) +line\n",
    "            result_lines.append(combined_line)\n",
    "            skip = 1\n",
    "        elif skip == 1:\n",
    "            last_string = line\n",
    "            skip = 0\n",
    "        else: \n",
    "            result_lines.append(last_string)\n",
    "            last_string = line\n",
    "           \n",
    "    final_poem = ' '.join(result_lines)\n",
    "\n",
    "    if final_poem.strip() == \"\":\n",
    "        return input_ids\n",
    "    else:\n",
    "        return tokenizer(final_poem, return_tensors='pt', padding=True).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aef347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замораживаем параметры модели\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dpo_loss_fn(preferred_logits, dispreferred_logits):\n",
    "    preferred_scores = preferred_logits.sum(dim=1)  \n",
    "    dispreferred_scores = dispreferred_logits.sum(dim=1)  \n",
    "    \n",
    "    score_diff = preferred_scores - dispreferred_scores \n",
    "    \n",
    "    loss = -F.logsigmoid(score_diff).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace36d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='222' max='222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [222/222 40:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1015.743200</td>\n",
       "      <td>16.325445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>972.634700</td>\n",
       "      <td>16.325445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>991.081300</td>\n",
       "      <td>16.325445</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=222, training_loss=968.683620143581, metrics={'train_runtime': 2426.9498, 'train_samples_per_second': 0.363, 'train_steps_per_second': 0.091, 'total_flos': 819114012573696.0, 'train_loss': 968.683620143581, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "    \n",
    "    preferred_input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    dispreferred_input_ids = generate_dispreferred_poem(model, preferred_input_ids, tokenizer)\n",
    "    \n",
    "    preferred_outputs = model(**inputs)\n",
    "    preferred_logits = preferred_outputs.logits\n",
    "    \n",
    "    dispreferred_outputs = model(input_ids=dispreferred_input_ids)\n",
    "    dispreferred_logits = dispreferred_outputs.logits\n",
    "\n",
    "    loss = dpo_loss_fn(preferred_logits, dispreferred_logits)\n",
    "    return loss\n",
    "\n",
    "trainer.training_step = training_step.__get__(trainer)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38084db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./poem_generator\\\\tokenizer_config.json',\n",
       " './poem_generator\\\\special_tokens_map.json',\n",
       " './poem_generator\\\\vocab.json',\n",
       " './poem_generator\\\\merges.txt',\n",
       " './poem_generator\\\\added_tokens.json',\n",
       " './poem_generator\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранение модели после обучения\n",
    "model.save_pretrained('./poem_generator')\n",
    "tokenizer.save_pretrained('./poem_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fdb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference_texts = list(poems.values()) \n",
    "\n",
    "generated_texts = []\n",
    "model.eval()  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(10): \n",
    "        random_index = random.randint(0, len(dataset) - 1)\n",
    "        random_poem = dataset[random_index]['text']  \n",
    "        \n",
    "        input_ids = tokenizer.encode(random_poem, return_tensors='pt', truncation=True, max_length=20)\n",
    "        \n",
    "        attention_mask = (input_ids != tokenizer.pad_token_id).long()\n",
    "        \n",
    "        output = model.generate(input_ids, attention_mask=attention_mask, max_length=60, temperature=0.7)\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea866ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['На камень жизни роковой\\nПриродою заброшен,\\nМладенец пылкий и живой\\n0|2|И , как в детстве , в детстве , в детстве , В детстве , в детстве , в детстве , в детстве , В детстве , в детстве , в детстве , в детстве ,', 'Как летней иногда порою\\nВдруг птичка в комнату влетит\\nИ жизнь и свет внесет в нее .|1|2|И в этом мире , где все на свете , И в этом мире , где все на свете , И в этом мире , где все на свете , И в', 'Уже великое небесное светило,\\nЛиюще с высоты обилие и свет,\\nНачертавший путь к нам , -|0|3|Ибо , как сказано в Евангелии , « не введи нас в искушение , но избавь нас от лукавого ». И', '\\nМолчи, скрывайся и таи\\nИ чувства и мечты свои –\\nПускай в них не осталось следа .|0|1|И пусть не будет больше слез .|1|1|И пусть не будет больше боли .|0|2|Молчи , скрывайся и', 'Как часто, бросив взор с утесистой вершины,\\nСажусь задумчивый в тени древес густой .|1|2|И , как в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в', 'На мир таинственный духо́в,\\nНад этой бездной безымянной,\\nПокровом ночи , над этой бездной ,|0|3|И , как в сказке , в тишине , В тишине , в тишине , в тишине , В тишине , в тишине , в тишине ,', 'Над виноградными холмами\\nПлывут златые облака.\\nВнизу зелеными волнами\\nШумят реки , и ветер шелестит .|1|1|Внизу , в долине , в долине ,|0|1|Внизу , в долине , в долине ,|1|', 'Она сидела на полу\\nИ груду писем разбирала –\\nИ, как остывшую золу , Срывала с них листы .|1|2|И , как будто в насмешку , Она , как будто в насмешку , Срывала с них листы .|0|2', 'Сей день, я помню, для меня\\nБыл утром жизненного дня:\\nСтояла молча предо мной .|1|2|И я , как в детстве , в слезах , не зная , что сказать , Я сказал : « Я люблю тебя ».|0|1|И я', 'Доехал исправно, усталый и целый,\\nСегодня прощаюсь со шляпою белой, И с тобой , и с тобой , и с тобой .|1|1|С тобой , с тобой , с тобой .|0|1|С тобой , с тобой , с тобой .']\n"
     ]
    }
   ],
   "source": [
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbb9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стихотворение ----------------\n",
      "В те дни кроваво-роковые,\n",
      "Когда, прервав борьбу свою,\n",
      "В ножнах , как в тисках ,\n",
      "В те дни кроваво - роковые , когда , прервав борьбу свою ,\n",
      "В ножнах , как в ти\n",
      "Стихотворение ----------------\n",
      "Давно ль, давно ль, о Юг блаженный,\n",
      "Я зрел тебя лицом к лицу ?\n",
      "Ты , как я , был , как я , и как ты , был .\n",
      "Ты , как я , был , как я , и как ты\n",
      "Стихотворение ----------------\n",
      "Не в первый раз волнуется Восток,\n",
      "Не в первый раз Христа там распинают,\n",
      "\n",
      "Не в первый раз в России гибнут люди .\n",
      "Не в первый раз в России гибнут люди .\n",
      "Не в первый раз в России\n",
      "Стихотворение ----------------\n",
      "Когда осьмнадцать лет твои\n",
      "И для тебя уж будут сновиденьем, –\n",
      "С любовью, с нежностью , с тоской , с тоской , –\n",
      "И , как в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в\n",
      "Стихотворение ----------------\n",
      "Спешу поздравить с неудачей:\n",
      "Она – блистательный успех,\n",
      "Для вас почетна наикрасивейшая .\n",
      "И , как я уже сказал , я не могу не заметить , что вы , как и я , не можете не заметить , что вы , как и\n",
      "Стихотворение ----------------\n",
      "Вот от моря и до моря\n",
      "Нить железная скользит,\n",
      "Много славы, много горя\n",
      "\n",
      "И в море , и в небе .\n",
      "И в море , и в небе .\n",
      "И в море , и в небе .\n",
      "\n",
      "Стихотворение ----------------\n",
      "Теперь не то, что за полгода,\n",
      "Теперь не тесный круг друзей –\n",
      "Сама великая страна ,\n",
      "И не то , что за полгода ,\n",
      "И не то , что за полгода ,\n",
      "И не то , что за полгода\n",
      "Стихотворение ----------------\n",
      "Так! Он спасен! Иначе быть не может!\n",
      "И чувство радости по Ру́си разлилось по всему телу .\n",
      "Он был счастлив .\n",
      "Он был счастлив .\n",
      "Он был счастлив .\n",
      "Он был счастлив\n",
      "Стихотворение ----------------\n",
      "Man muß die Slaven an die Mauer drücken.\n",
      "Они не знают , что такое \" свобода \".\n",
      "Они не знают , что такое \" свобода \" и \" демократия \". Они не знают , что такое \" свобода \".\n",
      "\n",
      "Стихотворение ----------------\n",
      "Впросонках слышу я – и не могу\n",
      "Вообразить такое сочетанье,\n",
      "А в голове – ни звука .\n",
      "И я , как в бреду , вижу , как в темноте ,\n",
      "И я , как в бреду , вижу ,\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'-\\|(.*?)\\|(.*?)\\|'\n",
    "\n",
    "def clean_and_replace(lines):\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line_replaced = re.sub(r'[|\\d]+', '\\n', line)\n",
    "        cleaned_lines.append(line_replaced)\n",
    "    return cleaned_lines\n",
    "\n",
    "cleaned_texts = clean_and_replace(generated_texts)\n",
    "for t in cleaned_texts:\n",
    "    print(\"Стихотворение ----------------\")\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc6c0cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стихотворение ----------------\n",
      "В те дни кроваво-роковые,\n",
      "Когда, прервав борьбу свою,\n",
      "В ножнах , как в тисках ,|0|3|В те дни кроваво - роковые , когда , прервав борьбу свою ,|1|3|В ножнах , как в ти\n",
      "Стихотворение ----------------\n",
      "Давно ль, давно ль, о Юг блаженный,\n",
      "Я зрел тебя лицом к лицу ?|1|2|Ты , как я , был , как я , и как ты , был .|0|2|Ты , как я , был , как я , и как ты\n",
      "Стихотворение ----------------\n",
      "Не в первый раз волнуется Восток,\n",
      "Не в первый раз Христа там распинают,\n",
      "0|1|Не в первый раз в России гибнут люди .|1|1|Не в первый раз в России гибнут люди .|0|1|Не в первый раз в России\n",
      "Стихотворение ----------------\n",
      "Когда осьмнадцать лет твои\n",
      "И для тебя уж будут сновиденьем, –\n",
      "С любовью, с нежностью , с тоской , с тоской , –|0|3|И , как в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в детстве , в\n",
      "Стихотворение ----------------\n",
      "Спешу поздравить с неудачей:\n",
      "Она – блистательный успех,\n",
      "Для вас почетна наикрасивейшая .|1|2|И , как я уже сказал , я не могу не заметить , что вы , как и я , не можете не заметить , что вы , как и\n",
      "Стихотворение ----------------\n",
      "Вот от моря и до моря\n",
      "Нить железная скользит,\n",
      "Много славы, много горя\n",
      "0|1|И в море , и в небе .|1|1|И в море , и в небе .|0|1|И в море , и в небе .|1\n",
      "Стихотворение ----------------\n",
      "Теперь не то, что за полгода,\n",
      "Теперь не тесный круг друзей –\n",
      "Сама великая страна ,|0|1|И не то , что за полгода ,|1|1|И не то , что за полгода ,|0|1|И не то , что за полгода\n",
      "Стихотворение ----------------\n",
      "Так! Он спасен! Иначе быть не может!\n",
      "И чувство радости по Ру́си разлилось по всему телу .|1|1|Он был счастлив .|0|1|Он был счастлив .|1|1|Он был счастлив .|0|1|Он был счастлив\n",
      "Стихотворение ----------------\n",
      "Man muß die Slaven an die Mauer drücken.\n",
      "Они не знают , что такое \" свобода \".|0|2|Они не знают , что такое \" свобода \" и \" демократия \". Они не знают , что такое \" свобода \".|1|1|\n",
      "Стихотворение ----------------\n",
      "Впросонках слышу я – и не могу\n",
      "Вообразить такое сочетанье,\n",
      "А в голове – ни звука .|1|2|И я , как в бреду , вижу , как в темноте ,|0|2|И я , как в бреду , вижу ,\n"
     ]
    }
   ],
   "source": [
    "for gen_poem in generated_texts:\n",
    "    print(\"Стихотворение ----------------\")\n",
    "    print(gen_poem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee055052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def calculate_chrf(reference_texts, generated_texts):\n",
    "    chrf_score = sacrebleu.corpus_chrf(generated_texts, reference_texts, char_order=6)\n",
    "    return chrf_score.score\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, texts):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(texts)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    return perplexity.item()\n",
    "\n",
    "def calculate_novelty(reference_texts, generated_texts):\n",
    "    reference_set = set(reference_texts)\n",
    "    unique_generated = set(generated_texts)\n",
    "    \n",
    "    novelty_score = len(unique_generated - reference_set) / len(unique_generated) if unique_generated else 0\n",
    "    return novelty_score\n",
    "\n",
    "def calculate_distinct_n(generated_texts, n=2):\n",
    "    ngrams = set()\n",
    "    for text in generated_texts:\n",
    "        tokens = text.split()  \n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.add(ngram)\n",
    "    \n",
    "    distinct_n_score = len(ngrams) / sum(len(text.split()) for text in generated_texts)\n",
    "    return distinct_n_score\n",
    "\n",
    "def llm_as_judge(model, tokenizer, texts):\n",
    "    scores = []\n",
    "    pattern = re.compile(r'\\d+') \n",
    "\n",
    "    for text in texts:\n",
    "        prompt = f\"Оцените качество следующего стихотворения от 1 до 10:\\n{text}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        score_str = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        match = pattern.search(score_str)\n",
    "        if match:\n",
    "            score = int(match.group())\n",
    "            score = max(1, min(10, score))\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            print(\"Не удалось найти оценку в ответе.\")\n",
    "            scores.append(5)\n",
    "\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    return average_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, reference_texts, generated_texts):\n",
    "    chrf_score = calculate_chrf(reference_texts, generated_texts)\n",
    "    perplexity_score = calculate_perplexity(model, tokenizer, generated_texts)\n",
    "    distinct_n_score = calculate_distinct_n(generated_texts)\n",
    "    novelty_score = calculate_novelty(reference_texts, generated_texts)\n",
    "    llm_judge_score = llm_as_judge(model, tokenizer, generated_texts)\n",
    "\n",
    "    print(f\"chrF++: {chrf_score:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity_score:.4f}\")\n",
    "    print(f\"Distinct-n: {distinct_n_score:.4f}\")\n",
    "    print(f\"Novelty: {novelty_score:.4f}\")\n",
    "    print(f\"LLM-as-judge score: {llm_judge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b65d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_10_texts = [row['text'] for row in dataset.select(range(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09628124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrF++: 3.8226\n",
      "Perplexity: 165.1382\n",
      "Distinct-n: 0.5678\n",
      "Novelty: 1.0000\n",
      "LLM-as-judge score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, tokenizer, reference_10_texts, cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea56e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poem(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=100, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a530b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sin2x\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sin2x\\.cache\\huggingface\\hub\\models--ai-forever--rugpt3small_based_on_gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "modelWithoutDPO = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79d208a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напиши стихотворение про лес,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в лесу, и в лесу,\n",
      "\t\tИ в\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Напиши стихотворение про лес\"\n",
    "print(generate_poem(modelWithoutDPO, tokenizer, prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
